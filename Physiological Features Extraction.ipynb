{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Physiological Features Extraction.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ5gQrSlXUhm",
        "outputId": "f585445a-700b-4964-b351-01d3fad2bd5d"
      },
      "source": [
        "#!pip install psycopg2-binary #for Michelle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: psycopg2-binary in c:\\users\\mnigc\\anaconda3\\lib\\site-packages (2.8.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSa0IjrNXUhq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, glob\n",
        "import sqlalchemy\n",
        "import sqlite3\n",
        "from sqlite3 import Error\n",
        "import sklearn as sk\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm, metrics\n",
        "#from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import math\n",
        "import gzip\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import psycopg2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-a1EQ7JXUhr"
      },
      "source": [
        "# Connecting to mimic postgres database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC29lZG6XUhs"
      },
      "source": [
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Postgres username, password, database name\n",
        "# @team - you'll need to change this according to what your Postgres info is under the Properties tab of your server in PgAdmin\n",
        "\n",
        "# Michelle's info\n",
        "#POSTGRES_ADDRESS = '127.0.0.1'\n",
        "#POSTGRES_PORT= '5432'\n",
        "#POSTGRES_USERNAME= 'postgres'\n",
        "#POSTGRES_PASSWORD= 'password'\n",
        "#POSTGRES_DBNAME = 'mimic'\n",
        "#postgres_str = ('postgresql://{username}:{password}@{ipaddress}:{port}/{dbname}'.format(username=POSTGRES_USERNAME,password=POSTGRES_PASSWORD,ipaddress=POSTGRES_ADDRESS,port=POSTGRES_PORT,dbname=POSTGRES_DBNAME))\n",
        "\n",
        "# Paroma's info \n",
        "POSTGRES_ADDRESS = 'localhost'   #127.0.0.1\n",
        "POSTGRES_PORT= '5432'\n",
        "POSTGRES_USERNAME= 'MacUser'\n",
        "POSTGRES_PASSWORD= ''\n",
        "POSTGRES_DBNAME = 'mimic'\n",
        "\n",
        "# putting in info\n",
        "postgres_str = ('postgresql://{username}:{password}@{ipaddress}:{port}/{dbname}?options=-csearch_path%3Dmimiciii'.format(username=POSTGRES_USERNAME,password=POSTGRES_PASSWORD,ipaddress=POSTGRES_ADDRESS,port=POSTGRES_PORT,dbname=POSTGRES_DBNAME))\n",
        "\n",
        "# Create the connection\n",
        "cnx = create_engine(postgres_str)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epHXFxGqXUhs",
        "outputId": "e1fa6fed-b761-4bfd-8f3b-c20faec0bf3d"
      },
      "source": [
        "# path with all un-normalized data - chartevents, lab events, procedure events\n",
        "path = \"/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/\"\n",
        "\n",
        "# For patient ids that meet our inclusion/exclusion criteria\n",
        "# loading all files from path with labevent name\n",
        "cohort_files = glob.glob(os.path.join(path, \"matched_ids_03032021.csv\"))\n",
        "#print(cohort_files)\n",
        "\n",
        "# initializing data frame\n",
        "cohort_df = []\n",
        "\n",
        "# loading all files into data frame\n",
        "for f in cohort_files:\n",
        "    df = pd.read_csv(f)\n",
        "    df['file'] = f.split('/')[-1]\n",
        "    cohort_df.append(df)\n",
        "\n",
        "# concatenating files into single data frame    \n",
        "cohort = pd.concat(cohort_df, ignore_index=True)\n",
        "print(cohort.head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\mnigc\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69ZYNcmxW_pn"
      },
      "source": [
        "path1 = \"/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/Un-normalized_matched_patient_id_Data/\"\n",
        "\n",
        "# For lab events\n",
        "# loading all files from path with labevent name\n",
        "cohort_files1 = glob.glob(os.path.join(path1, \"lab_events*\"))\n",
        "#print(cohort_files)\n",
        "\n",
        "# initializing data frame\n",
        "cohort_df1 = []\n",
        "\n",
        "# loading all files into data frame\n",
        "for f1 in cohort_files1:\n",
        "    df1 = pd.read_csv(f1)\n",
        "    df1['file'] = f1.split('/')[-1]\n",
        "    cohort_df1.append(df1)\n",
        "\n",
        "# concatenating files into single data frame    \n",
        "cohort1 = pd.concat(cohort_df1, ignore_index=True)\n",
        "print(cohort1.head(5))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m687hdJQXUht",
        "outputId": "96bd5bd1-ccd1-4759-b50b-16ef8f92f17e"
      },
      "source": [
        "#  getting subject ids from cohort files\n",
        "#patient_ids = cohort['subject_id']\n",
        "dead_ids = cohort['Subject_id_dead'];\n",
        "alive_ids = cohort['Subject_id_alive'];\n",
        "\n",
        "dead_ids = dead_ids[~np.isnan(dead_ids)]; # 483 dead patients\n",
        "alive_ids = alive_ids[~np.isnan(alive_ids)]; # 2114 alive patients\n",
        "\n",
        "unique_ids = pd.concat([alive_ids,dead_ids])\n",
        "unique_ids.drop_duplicates()\n",
        "print(unique_ids) #2597 patient ids\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2596"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O84xvaHKXUhu"
      },
      "source": [
        "# Feature extraction and cleaning by group"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALHVwhn7XWd7"
      },
      "source": [
        "# 03/04/21: Find prevalence of itemid\n",
        "# Michelle and Zhiwei's definition:  \"how often does this lab come up\"\n",
        "\n",
        "# Export prevalence of lab itemids into csv \n",
        "item_ids = pd.read_sql_query('''SELECT itemid, label, fluid \n",
        "FROM d_labitems WHERE\n",
        "label LIKE '%Albumin%' OR\n",
        "label LIKE '%Anion%' OR \n",
        "label LIKE '%Bilirubin%' OR\n",
        "label LIKE '%Chloride%' OR \n",
        "label LIKE '%Count%'OR\n",
        "label LIKE '%Creatinine%' OR\n",
        "label LIKE '%Glucose%' OR\n",
        "label LIKE '%Hemoglobin%' OR \n",
        "label LIKE '%Lactate%' OR \n",
        "label LIKE '%Oxygen Saturation%' OR\n",
        "label LIKE '%pH%' OR\n",
        "label LIKE '%Platelet%' OR\n",
        "label LIKE '%Potassium%' OR  \n",
        "label LIKE '%Sodium%' OR \n",
        "label LIKE '%Temperature%' \n",
        "AND label NOT LIKE '%Reticulocyte%';''',cnx)\n",
        "item_ids = item_ids['itemid']\n",
        "\n",
        "# Make empty pandas dataframe for prevalence\n",
        "prevalence = pd.DataFrame(index=item_ids, columns = ['prevalence'])\n",
        "prevalence = prevalence.fillna(0)\n",
        "#print(prevalence)\n",
        "\n",
        "# For every itemid find the prevalence\n",
        "# There will be an intermediate dataframe to help determine prevalence\n",
        "# NEVER loop with pandas dataframe\n",
        "# need to link icustays to lab id\n",
        "# lowest 40%, highest 80%\n",
        "\n",
        "lab_events = cohort1;\n",
        "\n",
        "for i in item_ids:\n",
        "    temp_lab = lab_events[lab_events['itemid'] == i]\n",
        "    temp_lab = temp_lab['subject_id'].drop_duplicates()\n",
        "    prev = temp_lab.shape[0]/unique_ids.shape[0]\n",
        "    prevalence.loc[i, 'prevalence'] = prev\n",
        "print(prevalence)    \n",
        "    \n",
        "# Export prevalence dataframe to csv file\n",
        "prevalence.to_csv(r'/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/prevalence_lab_itemids.csv', index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH4CWaNiXYw8"
      },
      "source": [
        "# Export label and fluids data into csv\n",
        "item_ids1 = pd.read_sql_query('''SELECT label, fluid, category\n",
        "FROM d_labitems WHERE\n",
        "label LIKE '%Albumin%' OR\n",
        "label LIKE '%Anion%' OR \n",
        "label LIKE '%Bilirubin%' OR\n",
        "label LIKE '%Chloride%' OR \n",
        "label LIKE '%Count%'OR\n",
        "label LIKE '%Creatinine%' OR\n",
        "label LIKE '%Glucose%' OR\n",
        "label LIKE '%Hemoglobin%' OR \n",
        "label LIKE '%Lactate%' OR \n",
        "label LIKE '%Oxygen Saturation%' OR\n",
        "label LIKE '%pH%' OR\n",
        "label LIKE '%Platelet%' OR\n",
        "label LIKE '%Potassium%' OR  \n",
        "label LIKE '%Sodium%' OR \n",
        "label LIKE '%Temperature%' \n",
        "AND label NOT LIKE '%Reticulocyte%';''',cnx)\n",
        "\n",
        "item_ids1.to_csv(r'/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/lab_labels_fluids_category.csv', index=False)\n",
        "\n",
        "print('Done exporting csv!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soaft8iXXdQG"
      },
      "source": [
        "## Load in itemid, prevalence data from prevalence_lab_itemids.csv file\n",
        "labflu = pd.read_csv(r\"/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/lab_labels_fluids_category.csv\")\n",
        "prev = pd.read_csv(r\"/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/prevalence_lab_itemids.csv\")\n",
        "\n",
        "# Join label, fluid, itemid, prevalence into one dataframe\n",
        "prevalence_frame = pd.concat([labflu, prev], axis=1, join='inner')\n",
        "#print(prevalence_frame)\n",
        "\n",
        "# Take out rows where prevalence < 0.7\n",
        "prevalence_filtered = prevalence_frame[prevalence_frame['prevalence'] >= 0.7]\n",
        "the_item_ids = prevalence_filtered['itemid']\n",
        "print(prevalence_filtered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzM1VWY5Xf2Q"
      },
      "source": [
        "# Find min, max, mean, stdev for each patient's lab event\n",
        "# csv per feature set\n",
        "    # make sure in the same order \n",
        "    # join later\n",
        "# This should all be really fast\n",
        "    # each lab should take a couple seconds\n",
        "\n",
        "for j in the_item_ids:\n",
        "    temp_lab1 = lab_events[lab_events['itemid'] == j].reset_index(drop=True) \n",
        "    temp_lab1 = temp_lab1[temp_lab1['subject_id'].isin(unique_ids)]\n",
        "    temp_itemid = temp_lab1['itemid']\n",
        "    \n",
        "    temp_lab1 = temp_lab1.drop(['itemid','charttime', 'flag','file'], axis=1)\n",
        "    temp_stats1 = temp_lab1.groupby(['subject_id']).agg(['min', 'max','mean','std'])\n",
        "    temp_stats1 = temp_stats1.reset_index(drop = True) \n",
        "    \n",
        "    combine = pd.concat([temp_lab1['subject_id'], temp_itemid1, temp_stats1],axis=1, join='inner')\n",
        "    \n",
        "    label = prevalence_frame.loc[prevalence_frame['itemid'] == j, 'label'].iloc[0]\n",
        "    fluid = prevalence_frame.loc[prevalence_frame['itemid'] == j, 'fluid'].iloc[0]\n",
        "    category = prevalence_frame.loc[prevalence_frame['itemid'] == j, 'category'].iloc[0]\n",
        "    \n",
        "    path2 = r'/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/lab_events_files/{}-{}-{}-{}.csv'.format(label, fluid, category, j)\n",
        "    combine.to_csv(path2, index=False)\n",
        "    print(path2)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw-dtPyga9_U"
      },
      "source": [
        "# Read in all of the lab ids and concat into 1 pandas dataframe\n",
        "path3 = \"/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/lab_events_files/\"\n",
        "\n",
        "cohort_files2 = glob.glob(os.path.join(path3, \"*.csv\"))\n",
        "#print(cohort_files2)\n",
        "\n",
        "# initializing data frame\n",
        "cohort_df2 = []\n",
        "\n",
        "# loading all files into data frame\n",
        "for f2 in cohort_files2:\n",
        "    df2 = pd.read_csv(f2)\n",
        "    df2['file'] = f2.split('/')[-1]\n",
        "    cohort_df2.append(df2)\n",
        "\n",
        "# concatenating files into single data frame    \n",
        "cohort2 = pd.concat(cohort_df2, ignore_index=True)\n",
        "lab_stats = cohort2;\n",
        "print(lab_stats.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfObwY54cL_K"
      },
      "source": [
        "# Read in all of the lab ids , normalize + impute + concat into 1 pandas dataframe\n",
        "\n",
        "path3 = \"C:/Users/Safiya/Downloads/lab_events_files\"\n",
        "\n",
        "def normalize_impute(dataset): # normalise and impute nan to mean  function\n",
        "    dataNorm=((dataset-dataset.min())/(dataset.max()-dataset.min()))\n",
        "    dataNorm['subject_id']=dataset['subject_id']\n",
        "    dataNorm['itemid'] = dataset['itemid']\n",
        "    dataset = dataNorm\n",
        "    dataset.apply(pd.to_numeric, errors='ignore')\n",
        "    dataset = dataset.fillna(dataset.mean())\n",
        "#same result as sklearn simpleimpute mean strategy \n",
        "    dataset['file'] = fname.split('/')[-1]\n",
        "    print(dataset.head())\n",
        "    cohort2.append(dataset)\n",
        "    return dataset\n",
        "\n",
        "cohort2 =[]\n",
        "path4 = \"C:/Users/Safiya/Downloads/lab_events_files/*.csv\"\n",
        "for fname in glob.glob(path4):\n",
        "    dataset = pd.read_csv(fname)\n",
        "    normalize_impute(dataset)\n",
        "    \n",
        "cohort2_df = pd.concat(cohort2, ignore_index=True)\n",
        "lab_stats = cohort2_df;\n",
        "print(lab_stats)\n",
        "path5 = \"C:/Users/Safiya/Downloads/lab_events_files_normalized/normalized_imputed_lab_stats.csv\"\n",
        "lab_stats.to_csv(path5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}