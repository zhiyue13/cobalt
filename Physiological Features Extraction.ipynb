{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Physiological Features Extraction.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ5gQrSlXUhm",
        "outputId": "f585445a-700b-4964-b351-01d3fad2bd5d"
      },
      "source": [
        "#!pip install psycopg2-binary #for Michelle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: psycopg2-binary in c:\\users\\mnigc\\anaconda3\\lib\\site-packages (2.8.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSa0IjrNXUhq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, glob\n",
        "import sqlalchemy\n",
        "import sqlite3\n",
        "from sqlite3 import Error\n",
        "import sklearn as sk\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm, metrics\n",
        "#from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import math\n",
        "import gzip\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import psycopg2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-a1EQ7JXUhr"
      },
      "source": [
        "# Connecting to mimic postgres database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC29lZG6XUhs"
      },
      "source": [
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Postgres username, password, database name\n",
        "# @team - you'll need to change this according to what your Postgres info is under the Properties tab of your server in PgAdmin\n",
        "\n",
        "# Michelle's info\n",
        "#POSTGRES_ADDRESS = '127.0.0.1'\n",
        "#POSTGRES_PORT= '5432'\n",
        "#POSTGRES_USERNAME= 'postgres'\n",
        "#POSTGRES_PASSWORD= 'password'\n",
        "#POSTGRES_DBNAME = 'mimic'\n",
        "#postgres_str = ('postgresql://{username}:{password}@{ipaddress}:{port}/{dbname}'.format(username=POSTGRES_USERNAME,password=POSTGRES_PASSWORD,ipaddress=POSTGRES_ADDRESS,port=POSTGRES_PORT,dbname=POSTGRES_DBNAME))\n",
        "\n",
        "# Paroma's info \n",
        "POSTGRES_ADDRESS = 'localhost'   #127.0.0.1\n",
        "POSTGRES_PORT= '5432'\n",
        "POSTGRES_USERNAME= 'MacUser'\n",
        "POSTGRES_PASSWORD= ''\n",
        "POSTGRES_DBNAME = 'mimic'\n",
        "\n",
        "# putting in info\n",
        "postgres_str = ('postgresql://{username}:{password}@{ipaddress}:{port}/{dbname}?options=-csearch_path%3Dmimiciii'.format(username=POSTGRES_USERNAME,password=POSTGRES_PASSWORD,ipaddress=POSTGRES_ADDRESS,port=POSTGRES_PORT,dbname=POSTGRES_DBNAME))\n",
        "\n",
        "# Create the connection\n",
        "cnx = create_engine(postgres_str)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epHXFxGqXUhs",
        "outputId": "e1fa6fed-b761-4bfd-8f3b-c20faec0bf3d"
      },
      "source": [
        "# path with all un-normalized data - chartevents, lab events, procedure events\n",
        "path = \"/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/\"\n",
        "\n",
        "# For patient ids that meet our inclusion/exclusion criteria\n",
        "# loading all files from path with labevent name\n",
        "cohort_files = glob.glob(os.path.join(path, \"matched_ids_03032021.csv\"))\n",
        "#print(cohort_files)\n",
        "\n",
        "# initializing data frame\n",
        "cohort_df = []\n",
        "\n",
        "# loading all files into data frame\n",
        "for f in cohort_files:\n",
        "    df = pd.read_csv(f)\n",
        "    df['file'] = f.split('/')[-1]\n",
        "    cohort_df.append(df)\n",
        "\n",
        "# concatenating files into single data frame    \n",
        "cohort = pd.concat(cohort_df, ignore_index=True)\n",
        "print(cohort.head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\mnigc\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69ZYNcmxW_pn"
      },
      "source": [
        "path1 = \"/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/Un-normalized_matched_patient_id_Data/\"\n",
        "\n",
        "# For lab events\n",
        "# loading all files from path with labevent name\n",
        "cohort_files1 = glob.glob(os.path.join(path1, \"lab_events*\"))\n",
        "#print(cohort_files)\n",
        "\n",
        "# initializing data frame\n",
        "cohort_df1 = []\n",
        "\n",
        "# loading all files into data frame\n",
        "for f1 in cohort_files1:\n",
        "    df1 = pd.read_csv(f1)\n",
        "    df1['file'] = f1.split('/')[-1]\n",
        "    cohort_df1.append(df1)\n",
        "\n",
        "# concatenating files into single data frame    \n",
        "cohort1 = pd.concat(cohort_df1, ignore_index=True)\n",
        "print(cohort1.head(5))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m687hdJQXUht",
        "outputId": "96bd5bd1-ccd1-4759-b50b-16ef8f92f17e"
      },
      "source": [
        "#  getting subject ids from cohort files\n",
        "#patient_ids = cohort['subject_id']\n",
        "dead_ids = cohort['Subject_id_dead'];\n",
        "alive_ids = cohort['Subject_id_alive'];\n",
        "\n",
        "dead_ids = dead_ids[~np.isnan(dead_ids)]; # 483 dead patients\n",
        "alive_ids = alive_ids[~np.isnan(alive_ids)]; # 2114 alive patients\n",
        "\n",
        "unique_ids = pd.concat([alive_ids,dead_ids])\n",
        "unique_ids.drop_duplicates()\n",
        "print(unique_ids) #2597 patient ids\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2596"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O84xvaHKXUhu"
      },
      "source": [
        "# Feature extraction and cleaning by group"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALHVwhn7XWd7"
      },
      "source": [
        "# 03/04/21: Find prevalence of itemid\n",
        "# Michelle and Zhiwei's definition:  \"how often does this lab come up\"\n",
        "\n",
        "# Export prevalence of lab itemids into csv \n",
        "item_ids = pd.read_sql_query('''SELECT itemid, label, fluid \n",
        "FROM d_labitems WHERE\n",
        "label LIKE '%Albumin%' OR\n",
        "label LIKE '%Anion%' OR \n",
        "label LIKE '%Bilirubin%' OR\n",
        "label LIKE '%Chloride%' OR \n",
        "label LIKE '%Count%'OR\n",
        "label LIKE '%Creatinine%' OR\n",
        "label LIKE '%Glucose%' OR\n",
        "label LIKE '%Hemoglobin%' OR \n",
        "label LIKE '%Lactate%' OR \n",
        "label LIKE '%Oxygen Saturation%' OR\n",
        "label LIKE '%pH%' OR\n",
        "label LIKE '%Platelet%' OR\n",
        "label LIKE '%Potassium%' OR  \n",
        "label LIKE '%Sodium%' OR \n",
        "label LIKE '%Temperature%' \n",
        "AND label NOT LIKE '%Reticulocyte%';''',cnx)\n",
        "item_ids = item_ids['itemid']\n",
        "\n",
        "# Make empty pandas dataframe for prevalence\n",
        "prevalence = pd.DataFrame(index=item_ids, columns = ['prevalence'])\n",
        "prevalence = prevalence.fillna(0)\n",
        "#print(prevalence)\n",
        "\n",
        "# For every itemid find the prevalence\n",
        "# There will be an intermediate dataframe to help determine prevalence\n",
        "# NEVER loop with pandas dataframe\n",
        "# need to link icustays to lab id\n",
        "# lowest 40%, highest 80%\n",
        "\n",
        "lab_events = cohort1;\n",
        "\n",
        "for i in item_ids:\n",
        "    temp_lab = lab_events[lab_events['itemid'] == i]\n",
        "    temp_lab = temp_lab['subject_id'].drop_duplicates()\n",
        "    prev = temp_lab.shape[0]/unique_ids.shape[0]\n",
        "    prevalence.loc[i, 'prevalence'] = prev\n",
        "print(prevalence)    \n",
        "    \n",
        "# Export prevalence dataframe to csv file\n",
        "prevalence.to_csv(r'/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/prevalence_lab_itemids.csv', index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH4CWaNiXYw8"
      },
      "source": [
        "# Export label and fluids data into csv\n",
        "item_ids1 = pd.read_sql_query('''SELECT label, fluid, category\n",
        "FROM d_labitems WHERE\n",
        "label LIKE '%Albumin%' OR\n",
        "label LIKE '%Anion%' OR \n",
        "label LIKE '%Bilirubin%' OR\n",
        "label LIKE '%Chloride%' OR \n",
        "label LIKE '%Count%'OR\n",
        "label LIKE '%Creatinine%' OR\n",
        "label LIKE '%Glucose%' OR\n",
        "label LIKE '%Hemoglobin%' OR \n",
        "label LIKE '%Lactate%' OR \n",
        "label LIKE '%Oxygen Saturation%' OR\n",
        "label LIKE '%pH%' OR\n",
        "label LIKE '%Platelet%' OR\n",
        "label LIKE '%Potassium%' OR  \n",
        "label LIKE '%Sodium%' OR \n",
        "label LIKE '%Temperature%' \n",
        "AND label NOT LIKE '%Reticulocyte%';''',cnx)\n",
        "\n",
        "item_ids1.to_csv(r'/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/lab_labels_fluids_category.csv', index=False)\n",
        "\n",
        "print('Done exporting csv!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soaft8iXXdQG"
      },
      "source": [
        "## Load in itemid, prevalence data from prevalence_lab_itemids.csv file\n",
        "labflu = pd.read_csv(r\"/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/lab_labels_fluids_category.csv\")\n",
        "prev = pd.read_csv(r\"/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/prevalence_lab_itemids.csv\")\n",
        "\n",
        "# Join label, fluid, itemid, prevalence into one dataframe\n",
        "prevalence_frame = pd.concat([labflu, prev], axis=1, join='inner')\n",
        "#print(prevalence_frame)\n",
        "\n",
        "# Take out rows where prevalence < 0.7\n",
        "prevalence_filtered = prevalence_frame[prevalence_frame['prevalence'] >= 0.7]\n",
        "the_item_ids = prevalence_filtered['itemid']\n",
        "print(prevalence_filtered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzM1VWY5Xf2Q"
      },
      "source": [
        "# Find min, max, mean, stdev for each patient's lab event\n",
        "# csv per feature set\n",
        "    # make sure in the same order \n",
        "    # join later\n",
        "# This should all be really fast\n",
        "    # each lab should take a couple seconds\n",
        "\n",
        "for j in the_item_ids:\n",
        "    temp_lab1 = lab_events[lab_events['itemid'] == j].reset_index(drop=True) \n",
        "    temp_lab1 = temp_lab1[temp_lab1['subject_id'].isin(unique_ids)]\n",
        "    temp_itemid = temp_lab1['itemid']\n",
        "    \n",
        "    temp_lab1 = temp_lab1.drop(['itemid','charttime', 'flag','file'], axis=1)\n",
        "    temp_stats1 = temp_lab1.groupby(['subject_id']).agg(['min', 'max','mean','std'])\n",
        "    temp_stats1 = temp_stats1.reset_index(drop = True) \n",
        "    \n",
        "    combine = pd.concat([temp_lab1['subject_id'], temp_itemid1, temp_stats1],axis=1, join='inner')\n",
        "    \n",
        "    label = prevalence_frame.loc[prevalence_frame['itemid'] == j, 'label'].iloc[0]\n",
        "    fluid = prevalence_frame.loc[prevalence_frame['itemid'] == j, 'fluid'].iloc[0]\n",
        "    category = prevalence_frame.loc[prevalence_frame['itemid'] == j, 'category'].iloc[0]\n",
        "    \n",
        "    path2 = r'/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/lab_events_files/{}-{}-{}-{}.csv'.format(label, fluid, category, j)\n",
        "    combine.to_csv(path2, index=False)\n",
        "    print(path2)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw-dtPyga9_U"
      },
      "source": [
        "# Read in all of the lab ids and concat into 1 pandas dataframe\n",
        "path3 = \"/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/lab_events_files/\"\n",
        "\n",
        "cohort_files2 = glob.glob(os.path.join(path3, \"*.csv\"))\n",
        "#print(cohort_files2)\n",
        "\n",
        "# initializing data frame\n",
        "cohort_df2 = []\n",
        "\n",
        "# loading all files into data frame\n",
        "for f2 in cohort_files2:\n",
        "    df2 = pd.read_csv(f2)\n",
        "    df2['file'] = f2.split('/')[-1]\n",
        "    cohort_df2.append(df2)\n",
        "\n",
        "# concatenating files into single data frame    \n",
        "cohort2 = pd.concat(cohort_df2, ignore_index=True)\n",
        "lab_stats = cohort2;\n",
        "print(lab_stats.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfObwY54cL_K"
      },
      "source": [
        "# Read in all of the lab ids , normalize + impute + concat into 1 pandas dataframe\n",
        "\n",
        "path3 = \"C:/Users/Safiya/Downloads/lab_events_files\"\n",
        "\n",
        "def normalize_impute(dataset): # normalise and impute nan to mean  function\n",
        "    dataNorm=((dataset-dataset.min())/(dataset.max()-dataset.min()))\n",
        "    dataNorm['subject_id']=dataset['subject_id']\n",
        "    dataNorm['itemid'] = dataset['itemid']\n",
        "    dataset = dataNorm\n",
        "    dataset.apply(pd.to_numeric, errors='ignore')\n",
        "    dataset = dataset.fillna(dataset.mean())\n",
        "#same result as sklearn simpleimpute mean strategy \n",
        "    dataset['file'] = fname.split('/')[-1]\n",
        "    print(dataset.head())\n",
        "    cohort2.append(dataset)\n",
        "    return dataset\n",
        "\n",
        "cohort2 =[]\n",
        "path4 = \"C:/Users/Safiya/Downloads/lab_events_files/*.csv\"\n",
        "for fname in glob.glob(path4):\n",
        "    dataset = pd.read_csv(fname)\n",
        "    normalize_impute(dataset)\n",
        "    \n",
        "cohort2_df = pd.concat(cohort2, ignore_index=True)\n",
        "lab_stats = cohort2_df;\n",
        "print(lab_stats)\n",
        "path5 = \"C:/Users/Safiya/Downloads/lab_events_files_normalized/normalized_imputed_lab_stats.csv\"\n",
        "lab_stats.to_csv(path5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDTaLRdVxcvA"
      },
      "source": [
        "###############\n",
        "# Find intime for each of our matched patient ids\n",
        "patientz = pd.read_sql_query('''SELECT subject_id, hadm_id, icustay_id, intime FROM ICUSTAYS;''',cnx)\n",
        "patientz = patientz[patientz['subject_id'].isin(unique_ids)]\n",
        "patientz = patientz.rename(columns={'subject_id':'subject_id','hadm_id':'HADM_ID', 'icustay_id':'ICUSTAY_ID','intime':'INTIME'})\n",
        "print(patientz)\n",
        "patientz.to_csv(r'/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/matched_ids_intime_03162021.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvZ34xcl7xF-"
      },
      "source": [
        "Script from Kirby's email"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oAPNjn87woz"
      },
      "source": [
        "# referring to Kirby's SOFA score for MIMIC email\n",
        "\n",
        "# Import packages\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pandarallel import pandarallel\n",
        "\n",
        "# Declare filepaths\n",
        "# NEED TO ALTER THIS\n",
        "lead_time = 24; #sys.argv[1]\n",
        "obs_time = 24; #sys.argv[2]\n",
        "#pids_file = f'Dynamic Data/Input/MIMIC_relative_{lead_time}hr_lead_{obs_time}hr_obs_data_set.csv' ############\n",
        "pids_file = f'/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/matched_ids_intime_03162021.csv'\n",
        "# path = \"/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/\"\n",
        "lab_events_file = '/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/mimic-iii-clinical-database-1.4/LABEVENTS.csv'\n",
        "input_events_cv_file = '/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/mimic-iii-clinical-database-1.4/INPUTEVENTS_CV.csv'\n",
        "input_events_mv_file = '/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/mimic-iii-clinical-database-1.4/INPUTEVENTS_MV.csv'\n",
        "output_events_file = '/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/mimic-iii-clinical-database-1.4/OUTPUTEVENTS.csv'\n",
        "chart_events_file = '/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/mimic-iii-clinical-database-1.4/CHARTEVENTS.csv'\n",
        "proc_events_file = '/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/mimic-iii-clinical-database-1.4/PROCEDUREEVENTS_MV.csv'\n",
        "out_file = f'/Users/MacUser/Desktop/Fall_2020_Semester/Classes/Precision_Care_Medicine/DATA/MIMIC_relative_{lead_time}hr_lead_{obs_time}hr_obs_SOFA.csv'\n",
        "\n",
        "# Read in patient IDs\n",
        "pids = pd.read_csv(pids_file, parse_dates=['INTIME'])\n",
        "\n",
        "# Only keep HADM_ID, ICUSTAY_ID, and time entered\n",
        "pids_and_starts = pids[['HADM_ID', 'ICUSTAY_ID', 'INTIME']]\n",
        "\n",
        "# Declare relevant ITEMIDS\n",
        "lab_ids = [50885,50912,50813,51265,50821,50816]\n",
        "input_cv_ids = [30047,30120,30044,30119,30309,30127,30128,30051,30043,30307,30125,30046]\n",
        "input_mv_ids = [221906,221289,221749,222315,221662,227692]\n",
        "output_ids = [40055,43175,40069,40094,40715,40473,40085,40057,40056,40405,40428,40086,40096,40651, \\\n",
        "            226559,226560,226561,226584,226563,226564,226565,226567,226557,226558,227489,227488]\n",
        "ventilation_ids = [720,223849,223848,445,448,449,450,1340,1486,1600,224687,\\\n",
        "            639,654,681,682,683,684,224685,224684,224686,\\\n",
        "            218,436,535,444,224697,224695,224696,224746,224747,\\\n",
        "            221,1,1211,1655,2000,226873,224738,224419,224750,227187,\\\n",
        "            543,\\\n",
        "            5865,5866,224707,224709,224705,224706,\\\n",
        "            60,437,505,506,686,220339,224700,\\\n",
        "            3459,\\\n",
        "            501,502,503,224702,\\\n",
        "            223,667,668,669,670,671,672,\\\n",
        "            224701,\\\n",
        "            226732,467,640]\n",
        "chart_ids = ventilation_ids + [51,442,455,6701,220179,220050,\\\n",
        "                               456,52,6702,443,220052,220181,225312,\\\n",
        "                               615,618,220210,224690,\\\n",
        "                               3420,3422,190,223835,\\\n",
        "                               723,223900,454,223901,184,220739]\n",
        "proc_ids = [227194,225468,225477]\n",
        "\n",
        "# Define relevant functions\n",
        "# Generate the result of variables found in chart_events\n",
        "# Used for SBP, MBP, RESP, FiO2\n",
        "def result_chart_events(row, feature):\n",
        "    df = chart_events\n",
        "    df = df[df['ICUSTAY_ID'] == row['ICUSTAY_ID']]\n",
        "    df = df[(df['offset'] >= (row['end']-1440)) & (df['offset'] <= row['end'])]\n",
        "    if feature == \"sbp\":\n",
        "        ids = [51,442,455,6701,220179,220050]\n",
        "        df = df[df['ITEMID'].isin(ids)]\n",
        "        df = df[(df['VALUENUM'] > 0) & (df['VALUENUM'] < 400)]\n",
        "    elif feature == \"mbp\":\n",
        "        ids = [456,52,6702,443,220052,220181,225312]\n",
        "        df = df[df['ITEMID'].isin(ids)]\n",
        "        df = df[(df['VALUENUM'] > 0) & (df['VALUENUM'] < 300)]\n",
        "    elif feature == \"resp\":\n",
        "        ids = [615,618,220210,224690]\n",
        "        df = df[df['ITEMID'].isin(ids)]\n",
        "        df = df[(df['VALUENUM'] > 0) & (df['VALUENUM'] < 70)]\n",
        "    elif feature == \"fiO2\":\n",
        "        ids = [3420,3422,190,223835]\n",
        "        df = df[df['ITEMID'].isin(ids)]\n",
        "        cond1 = ((df['ITEMID'] == 190) & (df['VALUENUM'] > 0.2) & (df['VALUENUM'] < 1))\n",
        "        cond2 = ((df['ITEMID'] == 223835) & (df['VALUENUM'] > 0) & (df['VALUENUM'] <= 1))\n",
        "        cond3 = ((df['ITEMID'] == 223835) & (df['VALUENUM'] > 1) & (df['VALUENUM'] < 21))\n",
        "        cond4 = ((df['ITEMID'] == 223835) & (df['VALUENUM'] > 100))\n",
        "        df['VALUENUM'] = np.where(cond1,df['VALUENUM']*100,df['VALUENUM'])\n",
        "        df['VALUENUM'] = np.where(cond2,df['VALUENUM']*100,df['VALUENUM'])\n",
        "        df['VALUENUM'] = np.where(cond3,np.nan,df['VALUENUM'])\n",
        "        df['VALUENUM'] = np.where(cond4,np.nan,df['VALUENUM'])\n",
        "    else:\n",
        "        return np.nan\n",
        "    return np.amin(df.dropna(subset = ['VALUENUM'])['VALUENUM'])\n",
        "\n",
        "# Generate the result of GCS\n",
        "def result_gcs(row):\n",
        "    df = chart_events\n",
        "    df = df[df['ICUSTAY_ID'] == row['ICUSTAY_ID']]\n",
        "    df = df[(df['offset'] >= (row['end']-1440)) & (df['offset'] <= row['end'])]\n",
        "    df = df[df['ERROR'] == 0]\n",
        "\n",
        "    verbal = df[df['ITEMID'].isin([723, 223900])]\n",
        "    motor = df[df['ITEMID'].isin([454, 223901])]\n",
        "    eyes = df[df['ITEMID'].isin([184, 220739])]\n",
        "\n",
        "    min_verbal = np.amin(verbal.dropna(subset = ['VALUENUM'])['VALUENUM'])\n",
        "    min_motor = np.amin(motor.dropna(subset = ['VALUENUM'])['VALUENUM'])\n",
        "    min_eyes = np.amin(eyes.dropna(subset = ['VALUENUM'])['VALUENUM'])\n",
        "\n",
        "    return min_verbal + min_motor + min_eyes\n",
        "\n",
        "# Generate the result of variables found in lab_events\n",
        "# Used for PO2, FiO2, Bilirubin, Platelets, Creatinine, Lactate\n",
        "def result_lab_events(row, feature):\n",
        "    df = lab_events\n",
        "    df = df[df['ICUSTAY_ID'] == row['ICUSTAY_ID']]\n",
        "    df = df[(df['offset'] >= (row['end']-1440)) & (df['offset'] <= row['end'])]\n",
        "\n",
        "    if feature == \"bilirubin\":\n",
        "        df = df[df['ITEMID'] == 50885]\n",
        "        df = df[(df['VALUENUM'] > 0) & (df['VALUENUM'] < 150)]\n",
        "    elif feature == \"creatinine\":\n",
        "        df = df[df['ITEMID'] == 50912]\n",
        "        df = df[(df['VALUENUM'] > 0) & (df['VALUENUM'] <150)]\n",
        "    elif feature == \"lactate\":\n",
        "        df = df[df['ITEMID'] == 50813]\n",
        "        df = df[(df['VALUENUM'] > 0) & (df['VALUENUM'] < 50)]\n",
        "    elif feature == \"platelets\":\n",
        "        df = df[df['ITEMID'] == 51265]\n",
        "        df = df[(df['VALUENUM'] > 0) & (df['VALUENUM'] < 10000)]\n",
        "    elif feature == \"pO2\":\n",
        "        df = df[df['ITEMID'] == 50821]\n",
        "        df = df[(df['VALUENUM'] > 0) & (df['VALUENUM'] < 800)]\n",
        "    elif feature == \"fiO2\":\n",
        "        df = df[df['ITEMID'] == 50816]\n",
        "        df = df[(df['VALUENUM'] > 20) & (df['VALUENUM'] < 100)]\n",
        "        \n",
        "    return np.amin(df.dropna(subset = ['VALUENUM'])['VALUENUM'])\n",
        "\n",
        "# Generate the result of vasopressors\n",
        "def result_vasopressors(row):\n",
        "    df1 = input_events_cv\n",
        "    df2 = input_events_mv\n",
        "\n",
        "    df1 = df1[df1['ICUSTAY_ID'] == row['ICUSTAY_ID']]\n",
        "    df1 = df1[(df1['offset'] >= (row['end']-1440)) & (df1['offset'] <= row['end'])]\n",
        "\n",
        "    df2 = df2[df2['ICUSTAY_ID'] == row['ICUSTAY_ID']]\n",
        "    df2 = df2[(df2['offset'] >= (row['end']-1440)) & (df2['offset'] <= row['end'])]\n",
        "    df2 = df2[df2['STATUSDESCRIPTION'] != 'Rewritten']\n",
        "    \n",
        "    return ((len(df1)+len(df2)) > 0)\n",
        "\n",
        "# Generate the result of urine\n",
        "def result_urine(row):\n",
        "    df = output_events\n",
        "    \n",
        "    df = df[df['ICUSTAY_ID'] == row['ICUSTAY_ID']]\n",
        "    df = df[(df['offset'] >= (row['end']-1440)) & (df['offset'] <= row['end'])]\n",
        "\n",
        "    pos_df = df[df['ITEMID'] != 227488]\n",
        "    neg_df = df[df['ITEMID'] == 227488]\n",
        "\n",
        "    return (pos_df['VALUE'].sum() - neg_df['VALUE'].sum())\n",
        "\n",
        "# Generate the result of ventilator\n",
        "def result_ventilator():\n",
        "    end_mech_ids = [226732,467,640]\n",
        "    mech_ids = [i for i in ventilation_ids if i not in end_mech_ids]\n",
        "    oxy_vals = [\"Nasal cannula\",\"Face tent\",\"Aerosol-cool\",\"Trach mask\",\\\n",
        "                \"High flow neb\",\"Non-rebreather\",\"Venti mask\",\"Medium conc mask\",\\\n",
        "                \"T-piece\",\"High flow nasal cannula\",\"Ultrasonic neb\",\"Vapomist\"]\n",
        "    mech = ventilation_events[ventilation_events['ITEMID'].isin(mech_ids)]\n",
        "    end_mech = ventilation_events[ventilation_events['ITEMID'].isin(end_mech_ids)]\n",
        "    proc = proc_events.copy()\n",
        "    proc.rename(columns={'STARTTIME':'CHARTTIME'},inplace=True)\n",
        "    \n",
        "    # Get unique ventilation times\n",
        "    mech = mech[~((mech['ITEMID']==223848) & (mech['VALUE']=='Other'))]\n",
        "    mech = mech[['ICUSTAY_ID','CHARTTIME']]\n",
        "    mech['mech'] = 1\n",
        "    mech.drop_duplicates(inplace=True)\n",
        "    \n",
        "    # Get unique oxygen therapy/extubation times\n",
        "    end_mech = end_mech[((end_mech['ITEMID']==226732) & (end_mech['VALUE'].isin(oxy_vals)))]\n",
        "    end_mech = pd.concat([end_mech,proc])\n",
        "    end_mech = end_mech[['ICUSTAY_ID','CHARTTIME']]\n",
        "    end_mech['mech'] = 0\n",
        "    end_mech.drop_duplicates(inplace=True)\n",
        "    \n",
        "    # Combine and find changes\n",
        "    all_mech = pd.concat([mech,end_mech])\n",
        "    all_mech.sort_values(['ICUSTAY_ID','CHARTTIME'],ascending=True,inplace=True)\n",
        "\n",
        "    all_mech['last_mech'] = all_mech['mech'].shift(periods=1)\n",
        "    all_mech['last_id'] = all_mech['ICUSTAY_ID'].shift(periods=1)\n",
        "    all_mech['keep'] = ((all_mech['last_id'] != all_mech['ICUSTAY_ID']) | (all_mech['last_mech'] != all_mech['mech']))\n",
        "    all_mech = all_mech[all_mech['keep']==True]\n",
        "    \n",
        "    all_mech['mech_start'] = all_mech['CHARTTIME'].shift(periods=1)\n",
        "    all_mech['keep'] = ((all_mech['last_mech']==1) & (all_mech['mech']==0) & (all_mech['last_id']==all_mech['ICUSTAY_ID']))\n",
        "    all_mech = all_mech[all_mech['keep']]\n",
        "    all_mech.rename(columns={'CHARTTIME':'mech_end'},inplace=True)\n",
        "\n",
        "    intimes = pids_and_starts[['ICUSTAY_ID','INTIME']]\n",
        "    all_mech = all_mech.merge(intimes,on='ICUSTAY_ID',how='left')\n",
        "    all_mech['start_offset'] = (all_mech['mech_start'] - all_mech['INTIME']).dt.total_seconds()/60\n",
        "    all_mech['end_offset'] = (all_mech['mech_end'] - all_mech['INTIME']).dt.total_seconds()/60\n",
        "\n",
        "    lookup = pids.set_index('ICUSTAY_ID')\n",
        "    def keep_row(current_ID,start_offset,end_offset):\n",
        "        window_start = lookup.loc[current_ID,'start']\n",
        "        window_end = lookup.loc[current_ID,'end']\n",
        "        if (start_offset <= window_end):\n",
        "            if (end_offset <= window_start):\n",
        "                return False\n",
        "            else:\n",
        "                return True\n",
        "        else:\n",
        "            return False\n",
        "    all_mech['keep'] = all_mech.apply(lambda row: keep_row(row['ICUSTAY_ID'],row['start_offset'],row['end_offset']), axis=1)\n",
        "    all_mech = all_mech[all_mech['keep']==True]\n",
        "\n",
        "    pids['ventilator'] = pids['ICUSTAY_ID'].isin(all_mech['ICUSTAY_ID']).astype(int)\n",
        "    \n",
        "# Generate SOFA score and component features\n",
        "def SOFA_score(row):\n",
        "        \n",
        "    # Resp: (PaO2/FiO2, ventilation) into \"sofa_resp\"\n",
        "    sofa_resp = 0\n",
        "    if ((not np.isnan(row['pO2'])) and (not np.isnan(row['fiO2']))):\n",
        "        try:\n",
        "            paO2_fiO2_ratio = row['pO2']/row['fiO2']\n",
        "        except ZeroDivisionError:\n",
        "            paO2_fiO2_ratio = 1000\n",
        "        if (paO2_fiO2_ratio < 100 and row['ventilator']):\n",
        "            sofa_resp = 4\n",
        "        elif (paO2_fiO2_ratio < 200 and row['ventilator']):\n",
        "            sofa_resp = 3\n",
        "        elif (paO2_fiO2_ratio < 300):\n",
        "            sofa_resp = 2\n",
        "        elif (paO2_fiO2_ratio < 400):\n",
        "            sofa_resp = 1\n",
        "                \n",
        "    # Nervous: (GCS) into \"sofa_nervous\"\n",
        "    sofa_nervous = 0\n",
        "    if (not np.isnan(row['gcs'])):\n",
        "        if (row['gcs'] < 6):\n",
        "            sofa_nervous = 4\n",
        "        elif (row['gcs'] < 10 and row['gcs'] >= 6):\n",
        "            sofa_nervous = 3\n",
        "        elif (row['gcs'] < 13 and row['gcs'] >= 10):\n",
        "            sofa_nervous = 2\n",
        "        elif (row['gcs'] < 15 and row['gcs'] >= 13):\n",
        "            sofa_nervous = 1\n",
        "                \n",
        "    # Cardio: (MBP, vasopressors) into \"sofa_cardio\"\n",
        "    sofa_cardio = 0\n",
        "    if ((not np.isnan(row['mbp'])) and (row['mbp'] < 70)):\n",
        "        sofa_cardio = 1\n",
        "    if (row['vasopressors']):\n",
        "        sofa_cardio = 2\n",
        "        \n",
        "    # Liver: (bilirubin) into \"sofa_liver\"\n",
        "    sofa_liver = 0\n",
        "    if (not np.isnan(row['bilirubin'])):\n",
        "        if (row['bilirubin'] >= 12):\n",
        "            sofa_liver = 4\n",
        "        elif (row['bilirubin'] >= 6 and row['bilirubin'] < 12):\n",
        "            sofa_liver = 3\n",
        "        elif (row['bilirubin'] >= 2 and row['bilirubin'] < 6):\n",
        "            sofa_liver = 2\n",
        "        elif (row['bilirubin'] >= 1.2 and row['bilirubin'] < 2):\n",
        "            sofa_liver = 1\n",
        "                \n",
        "    # Coag: (platelets) into \"sofa_coag\"\n",
        "    sofa_coag = 0\n",
        "    if (not np.isnan(row['platelets'])):\n",
        "        if (row['platelets'] < 20):\n",
        "            sofa_coag = 4\n",
        "        elif (row['platelets'] < 50):\n",
        "            sofa_coag = 3\n",
        "        elif (row['platelets'] < 100):\n",
        "            sofa_coag = 2\n",
        "        elif (row['platelets'] < 150):\n",
        "            sofa_coag = 1\n",
        "                \n",
        "    # Kidneys: (creatinine and urine output) into \"sofa_kidney\"\n",
        "    sofa_kidney = 0\n",
        "    if (not np.isnan(row['creatinine'])):\n",
        "        if (row['creatinine'] >= 5):\n",
        "            sofa_kidney = 4\n",
        "        elif (row['creatinine'] >= 3.4 and row['creatinine'] < 5):\n",
        "            sofa_kidney = 3\n",
        "        elif (row['creatinine'] >= 2 and row['creatinine'] < 3.4):\n",
        "            sofa_kidney = 2\n",
        "        elif (row['creatinine'] >= 1.2 and row['creatinine'] < 2):\n",
        "            sofa_kidney = 1\n",
        "    elif (not np.isnan(row['urine'])):\n",
        "        if (row['urine'] <= 200):\n",
        "            sofa_kidney = 4\n",
        "        elif (row['urine'] <= 500):\n",
        "            sofa_kidney = 3\n",
        "                \n",
        "    temp_sofa_score = sofa_resp + sofa_nervous + sofa_cardio + sofa_liver + sofa_coag + sofa_kidney\n",
        "\n",
        "    return sofa_resp, sofa_nervous, sofa_cardio, sofa_liver, sofa_coag, sofa_kidney, temp_sofa_score\n",
        "\n",
        "# Generate qSOFA score and component features\n",
        "def qSOFA_score(row):\n",
        "    \n",
        "    # GCS\n",
        "    if row['sofa_nervous'] == 0:\n",
        "        altered_mental_state = False\n",
        "    else:\n",
        "        altered_mental_state = True\n",
        "        \n",
        "    # Resp\n",
        "    resp_rate = False\n",
        "    if ((not np.isnan(row['resp'])) and (row['resp'] >= 22)):\n",
        "        resp_rate = True\n",
        "        \n",
        "    # Systolic\n",
        "    sys_bp = False\n",
        "    if ((not np.isnan(row['sbp'])) and (row['sbp'] <= 100)):\n",
        "        sys_bp = True\n",
        "    \n",
        "    qSOFA = altered_mental_state + resp_rate + sys_bp\n",
        "    \n",
        "    return altered_mental_state, resp_rate, sys_bp, qSOFA\n",
        "\n",
        "# Generate sepsis and component features\n",
        "def sepsis(row):\n",
        "    \n",
        "    # Sepsis suspected\n",
        "    if (row['sofa_score'] >= 2 and row['qsofa_score'] >= 2):\n",
        "        suspected_sepsis = True\n",
        "    else:\n",
        "        suspected_sepsis = False\n",
        "\n",
        "    # Lactate\n",
        "    sepsis_lactate = False\n",
        "    if ((not np.isnan(row['lactate'])) and (row['lactate'] > 2)):\n",
        "        sepsis_lactate = True\n",
        "\n",
        "    # MBP\n",
        "    sepsis_map = False\n",
        "    if ((not np.isnan(row['mbp'])) and (row['mbp'] >= 65)):\n",
        "        sepsis_map = True\n",
        "\n",
        "    # Septic shock suspected\n",
        "    suspected_septic_shock = (suspected_sepsis and sepsis_lactate and sepsis_map)\n",
        "\n",
        "    return suspected_sepsis, sepsis_lactate, sepsis_map, suspected_septic_shock\n",
        "\n",
        "# Initialization\n",
        "pandarallel.initialize(progress_bar = False)\n",
        "\n",
        "# Read in relevant data files and generate associated features\n",
        "# Chart events\n",
        "chart_events = pd.read_csv(chart_events_file, parse_dates=['CHARTTIME'],\n",
        "\n",
        "                           usecols=['ICUSTAY_ID','ITEMID','CHARTTIME','VALUE','VALUENUM','ERROR'])\n",
        "chart_events = chart_events.merge(pids_and_starts, on='ICUSTAY_ID')\n",
        "chart_events = chart_events[chart_events['ITEMID'].isin(chart_ids)]\n",
        "chart_events['CHARTTIME'] = pd.to_datetime(chart_events['CHARTTIME'])\n",
        "chart_events['INTIME'] = pd.to_datetime(chart_events['INTIME'])\n",
        "chart_events['offset'] = (chart_events['CHARTTIME'] - chart_events['INTIME']).dt.total_seconds()/60\n",
        "ventilation_events = chart_events[chart_events['ITEMID'].isin(ventilation_ids)]\n",
        "\n",
        "pids['sbp'] = pids.parallel_apply(lambda row: result_chart_events(row, 'sbp'), axis=1)\n",
        "pids['mbp'] = pids.parallel_apply(lambda row: result_chart_events(row, 'mbp'), axis=1)\n",
        "pids['resp'] = pids.parallel_apply(lambda row: result_chart_events(row, 'resp'), axis=1)\n",
        "pids['fiO2_chart'] = pids.parallel_apply(lambda row: result_chart_events(row, 'fiO2'), axis=1)\n",
        "pids['gcs'] = pids.parallel_apply(lambda row: result_gcs(row), axis=1)\n",
        "del chart_events\n",
        "\n",
        "# Lab events\n",
        "lab_events = pd.read_csv(lab_events_file, parse_dates=['CHARTTIME'],\n",
        "                         usecols=['HADM_ID','ITEMID','CHARTTIME','VALUENUM'])\n",
        "lab_events = lab_events.merge(pids_and_starts, on='HADM_ID')\n",
        "lab_events = lab_events[lab_events['ITEMID'].isin(lab_ids)]\n",
        "lab_events['offset'] = (lab_events['CHARTTIME'] - lab_events['INTIME']).dt.total_seconds()/60\n",
        "\n",
        "pids['bilirubin'] = pids.parallel_apply(lambda row: result_lab_events(row, 'bilirubin'), axis=1)\n",
        "pids['creatinine'] = pids.parallel_apply(lambda row: result_lab_events(row, 'creatinine'), axis=1)\n",
        "pids['lactate'] = pids.parallel_apply(lambda row: result_lab_events(row, 'lactate'), axis=1)\n",
        "pids['platelets'] = pids.parallel_apply(lambda row: result_lab_events(row, 'platelets'), axis=1)\n",
        "pids['pO2'] = pids.parallel_apply(lambda row: result_lab_events(row, 'pO2'), axis=1)\n",
        "pids['fiO2_lab'] = pids.parallel_apply(lambda row: result_lab_events(row, 'fiO2'), axis=1)\n",
        "del lab_events\n",
        "\n",
        "# Input events CareVue/Metavision\n",
        "input_events_cv = pd.read_csv(input_events_cv_file, parse_dates=['CHARTTIME'],\n",
        "                              usecols=['ICUSTAY_ID','ITEMID','CHARTTIME','RATE'])\n",
        "input_events_cv = input_events_cv.merge(pids_and_starts, on='ICUSTAY_ID')\n",
        "input_events_cv = input_events_cv[input_events_cv['ITEMID'].isin(input_cv_ids)]\n",
        "input_events_cv = input_events_cv[input_events_cv['RATE'] > 0]\n",
        "input_events_cv['offset'] = (input_events_cv['CHARTTIME'] - input_events_cv['INTIME']).dt.total_seconds()/60\n",
        "\n",
        "input_events_mv = pd.read_csv(input_events_mv_file, parse_dates=['STARTTIME'],\n",
        "                              usecols=['ICUSTAY_ID','ITEMID','STARTTIME','RATE','STATUSDESCRIPTION'])\n",
        "input_events_mv = input_events_mv.merge(pids_and_starts, on='ICUSTAY_ID')\n",
        "input_events_mv = input_events_mv[input_events_mv['ITEMID'].isin(input_mv_ids)]\n",
        "input_events_mv = input_events_mv[input_events_mv['RATE'] > 0]\n",
        "input_events_mv['offset'] = (input_events_mv['STARTTIME'] - input_events_mv['INTIME']).dt.total_seconds()/60\n",
        "\n",
        "pids['vasopressors'] = pids.parallel_apply(lambda row: result_vasopressors(row), axis=1)\n",
        "del input_events_cv, input_events_mv\n",
        "\n",
        "# Output events\n",
        "output_events = pd.read_csv(output_events_file, parse_dates=['CHARTTIME'],\n",
        "                            usecols=['ICUSTAY_ID','ITEMID','CHARTTIME','VALUE'])\n",
        "output_events = output_events.merge(pids_and_starts, on='ICUSTAY_ID')\n",
        "output_events['offset'] = (output_events['CHARTTIME'] - output_events['INTIME']).dt.total_seconds()/60\n",
        "\n",
        "pids['urine'] = pids.parallel_apply(lambda row: result_urine(row), axis=1)\n",
        "del output_events\n",
        "\n",
        "# Procedure events Metavision\n",
        "proc_events = pd.read_csv(proc_events_file, parse_dates=['STARTTIME'],\n",
        "                          usecols = ['ICUSTAY_ID','STARTTIME','ITEMID'])\n",
        "proc_events = proc_events.merge(pids_and_starts, on='ICUSTAY_ID')\n",
        "proc_events = proc_events[proc_events['ITEMID'].isin(proc_ids)]\n",
        "proc_events['offset'] = (proc_events['STARTTIME'] - proc_events['INTIME']).dt.total_seconds()/60\n",
        "\n",
        "result_ventilator()\n",
        "del proc_events, ventilation_events\n",
        "\n",
        "# Consolidate fiO2\n",
        "pids['fiO2'] = pids.apply(lambda row: min(row['fiO2_chart'],row['fiO2_lab']), axis=1)\n",
        "pids.drop(columns=['fiO2_chart', 'fiO2_lab'], inplace = True)\n",
        "\n",
        "# Calculate scores\n",
        "pids['sofa_resp'], pids['sofa_nervous'], pids['sofa_cardio'], pids['sofa_liver'], pids['sofa_coag'], pids['sofa_kidney'], pids['sofa_score'] = zip(*pids.parallel_apply(lambda row: SOFA_score(row), axis=1))\n",
        "pids['qsofa_altered_mental'], pids['qsofa_resp_rate'], pids['qsofa_sys_bp'], pids['qsofa_score'] = zip(*pids.parallel_apply(lambda row: qSOFA_score(row), axis=1))\n",
        "pids['suspected_sepsis'], pids['sepsis_lactate'], pids['sepsis_map'], pids['suspected_septic_shock'] = zip(*pids.parallel_apply(lambda row: sepsis(row), axis=1))\n",
        "\n",
        "# Export to csv\n",
        "pids.to_csv(out_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}